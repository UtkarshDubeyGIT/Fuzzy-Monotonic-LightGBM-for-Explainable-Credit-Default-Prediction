% Elsevier-style merged manuscript: IEEE + EDA content
\documentclass[preprint,12pt]{elsarticle}

% -------------------- Packages --------------------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue, citecolor=blue}

% Graceful image inclusion: include placeholder box if file is missing
\newcommand{\maybeincludegraphic}[2][]{%
	\IfFileExists{#2}{\includegraphics[#1]{#2}}{\fbox{\scriptsize Missing figure: #2}}}

\journal{—}

% -------------------- Front matter --------------------
\begin{document}

\begin{frontmatter}

	itle{Fuzzy-Monotonic LightGBM for Explainable Credit Default Prediction: \\
An Elsevier-Style Merged Manuscript with Exploratory Data Analysis}

\author[inst1]{Utkarsh Dubey}
\author[inst1]{Kanav Singla}
\author[inst1]{Dushyant Bhardwaj}

\affiliation[inst1]{organization={Department of Computer Science, Netaji Subhas University of Technology},
						city={New Delhi},
						country={India}}

\begin{abstract}
Credit default prediction is challenging due to severe class imbalance, highly non-linear behavioral drivers, and regulatory requirements for transparency in high-stakes lending decisions. Modern boosted models (e.g., LightGBM) achieve strong performance but remain black-box in nature, while purely rule-based fuzzy systems are interpretable but weak at scale. This work proposes a hybrid \textbf{Fuzzy-Monotonic LightGBM} framework that combines domain-engineered financial indicators, fuzzy linguistic rule activations, and economically consistent monotonic constraints inside a unified gradient-boosted architecture. Experiments conducted on two standard benchmarks — the Taiwan dataset (predictive evaluation) and the German dataset (interpretability demonstration) — show that this hybrid model improves probability calibration, stabilizes PR-AUC behavior, and maintains directionally correct economic structure while preserving competitive discrimination performance. The results suggest that combining fuzzy reasoning with monotonic gradient boosting provides a regulator-aligned, practically deployable XAI framework for credit default prediction in real financial environments. In addition, we include a comprehensive exploratory data analysis (EDA) for both datasets to motivate design choices, assess data quality, and visualize risk patterns.
\end{abstract}

\begin{keyword}
credit risk scoring \sep fuzzy reasoning \sep monotonic models \sep explainable AI \sep LightGBM \sep calibration \sep credit default prediction
\end{keyword}

\end{frontmatter}

% =====================================================
% 1. Introduction
% =====================================================
\section{Introduction}
Financial institutions rely on data-driven risk scoring for loan approval, pricing, capital allocation, IFRS-9 staging, and early warning systems. While modern machine learning (ML) models (e.g., gradient boosting) outperform linear scorecards, their opacity hinders deployment in regulated environments where transparency and economically consistent behavior are mandatory. Conversely, rule-based fuzzy systems are interpretable but underfit at scale. This paper proposes a hybrid \textbf{Fuzzy-Monotonic LightGBM} framework that integrates: (i) engineered behavioral features, (ii) fuzzy linguistic memberships and rule activations, and (iii) monotonic constraints within LightGBM. The goal is to preserve strong predictive performance while ensuring structural interpretability and compliance with supervisory expectations.

We evaluate on two benchmarks: \textbf{Taiwan Credit Card Default} (primary predictive evaluation) and \textbf{German Credit} (interpretability demonstration). We also present a detailed EDA for both datasets, which informs preprocessing choices and highlights class imbalance, variable distributions, and risk segmentation patterns.

% =====================================================
% 2. Literature Review
% =====================================================
\section{Literature Review}
Classical credit scoring relies on logistic regression and scorecards due to interpretability and governance \cite{b1}. These models assume linear relationships and limited interactions. ML models such as Random Forests, SVMs, and Gradient Boosted Trees improve discrimination \cite{b2,b7,b11}, but are often rejected in production due to opacity and limited trust \cite{b3,b8}. Post-hoc methods (e.g., SHAP) provide attribution but remain non-structural and can conflict with economic intuition \cite{b9}. Monotonic constraints in tree models \cite{b4,b6,b10} inject directional priors yet do not inherently provide human-understandable reasoning. Fuzzy systems offer transparency via linguistic rules but typically underperform on high-dimensional behavioral data. This motivates a unified approach that blends fuzzy reasoning with monotonic boosting.

% =====================================================
% 3. Datasets and Roles
% =====================================================
\section{Datasets and Roles}
Two public benchmarks are used. The \textbf{Taiwan dataset} (30,000 clients) contains demographic attributes, credit limit, repayment history (PAY\_0..PAY\_6), bill statements (BILL\_AMT1..6), and repayments (PAY\_AMT1..6). The target is next-month default. The \textbf{German Credit} dataset (1,000 applicants) includes categorical attributes (e.g., housing, checking/saving accounts, purpose) and numerical attributes (age, credit amount, duration) with a binary risk target.

Dataset roles follow industry practice: Taiwan for primary modelling and ablations; German for interpretability and rule transparency.

% =====================================================
% 4. Exploratory Data Analysis (Merged)
% =====================================================
\section{Exploratory Data Analysis}
We summarize key EDA findings that informed feature engineering and modelling.

\subsection{German Credit Dataset}
\paragraph{Target distribution.} Figure~\ref{fig:german-risk-dist} shows a moderate imbalance with \emph{Good} borrowers in the majority, motivating metrics beyond accuracy.
\begin{figure}[H]
	\centering
	\maybeincludegraphic[width=0.6\textwidth]{latex_files/01_risk_distribution.png}
	\caption{Risk distribution (German dataset)}
	\label{fig:german-risk-dist}
\end{figure}

\paragraph{Missingness.} Most variables exhibit low missingness except \emph{Checking account} and \emph{Saving accounts}.
\begin{figure}[H]
	\centering
	\maybeincludegraphic[width=0.8\textwidth]{latex_files/00_missing_values_heatmap.png}
	\caption{Missing values percentage (German dataset)}
\end{figure}

\paragraph{Numeric distributions.} Right-skewness in \emph{Credit amount} and \emph{Duration} suggests log or robust scaling for sensitive models.
\begin{figure}[H]
	\centering
	\maybeincludegraphic[width=0.32\textwidth]{latex_files/hist_Age.png}
	\maybeincludegraphic[width=0.32\textwidth]{latex_files/hist_Credit amount.png}
	\maybeincludegraphic[width=0.32\textwidth]{latex_files/hist_Duration.png}
	\caption{Histograms (Age, Credit amount, Duration)}
\end{figure}

\paragraph{Numeric relationship with target.}
\begin{figure}[H]
	\centering
	\maybeincludegraphic[width=0.32\textwidth]{latex_files/num_box_Age_by_risk.png}
	\maybeincludegraphic[width=0.32\textwidth]{latex_files/num_box_Credit amount_by_risk.png}
	\maybeincludegraphic[width=0.32\textwidth]{latex_files/num_box_Duration_by_risk.png}
	\caption{Boxplots of numeric features by Risk (German)}
\end{figure}

\paragraph{Correlation and risk comparisons.}
\begin{figure}[H]
	\centering
	\maybeincludegraphic[width=0.7\textwidth]{latex_files/02_correlation_heatmap.png}
	\caption{Correlation heatmap (numeric features — German)}
\end{figure}

% Full-page categorical risk comparisons (copied layout)
\begin{figure}[p]
	\centering
	\captionsetup{font=small}
	\captionsetup[subfigure]{font=footnotesize, justification=centering}

	% Counts: Row 1
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Checking account.png}
		\caption{Counts: Checking account}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Housing.png}
		\caption{Counts: Housing}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Job.png}
		\caption{Counts: Job}
	\end{subfigure}

	\vspace{1.2em}

	% Counts: Row 2
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Purpose.png}
		\caption{Counts: Purpose}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Saving accounts.png}
		\caption{Counts: Saving accounts}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Sex.png}
		\caption{Counts: Sex}
	\end{subfigure}

	\vspace{1.2em}

	% Default rates: Row 3
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Checking account.png}
		\caption{Default rate: Checking account}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Housing.png}
		\caption{Default rate: Housing}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Job.png}
		\caption{Default rate: Job}
	\end{subfigure}

	\vspace{1.2em}

	% Default rates: Row 4
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Purpose.png}
		\caption{Default rate: Purpose}
	\end{subfigure}
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Saving accounts.png}
		\caption{Default rate: Saving account}
	\end{subfigure}
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Sex.png}
		\caption{Default rate: Sex}
	\end{subfigure}

	\caption{Risk comparison across categorical features (German dataset).}
\end{figure}

\clearpage

\paragraph{Violin plots.}
\begin{figure}[H]
	\centering
	\maybeincludegraphic[width=0.32\textwidth]{latex_files/violin_Age_by_risk.png}
	\maybeincludegraphic[width=0.32\textwidth]{latex_files/violin_Credit amount_by_risk.png}
	\maybeincludegraphic[width=0.32\textwidth]{latex_files/violin_Duration_by_risk.png}
	\caption{Violin plots (Age, Credit amount, Duration) by Risk}
\end{figure}

\paragraph{Chi-square summary (categorical association).}
\begin{table}[H]
\centering
\caption{Chi-Square Test Summary for Categorical Features (German Dataset)}
\label{tab:chi2_summary_german}
\begin{tabular}{lccc}
	oprule
	extbf{Feature} & \textbf{Chi-Square} & \textbf{df} & \textbf{p-value} \\
\midrule
Checking Account & 112.30 & 3 & $<0.001$ \\
Housing          & 25.80  & 2 & $<0.001$ \\
Purpose          & 8.60   & 5 & 0.123 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Taiwan Default Dataset}
\paragraph{Categorical distributions (sample).}
\begin{figure}[H]
	\centering
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X1.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X2.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X3.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X4.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X5.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X6.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X7.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X8.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X9.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X10.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X11.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X12.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X13.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X14.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X15.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X16.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X17.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X18.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X19.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X20.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X21.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X22.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_X23.png}
	\maybeincludegraphic[width=0.23\textwidth]{latex_files/bar_Y.png}
	\caption{Sample categorical distributions (Taiwan)}
\end{figure}

\paragraph{Correlation (all columns).}
\begin{figure}[H]
	\centering
	\maybeincludegraphic[width=0.9\textwidth]{latex_files/corr_heatmap_all_columns.png}
	\caption{Spearman correlation heatmap (all columns — Taiwan)}
\end{figure}

\paragraph{Class balance and IV summary.}
\begin{table}[H]
\centering
\caption{Class Balance Summary (Taiwan Dataset)}
\label{tab:class_balance_taiwan}
\begin{tabular}{lcc}
	oprule
	extbf{Class} & \textbf{Count} & \textbf{Percentage} \\
\midrule
Non-Default (0) & 23364 & 77.88\% \\
Default (1)     & 6636  & 22.12\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Information Value (IV) Summary}
\label{tab:iv_summary_taiwan}
\begin{tabular}{lcc}
	oprule
	extbf{Feature} & \textbf{IV} & \textbf{Strength} \\
\midrule
PAY\_0        & 0.54 & Strong \\
PAY\_2        & 0.42 & Strong \\
Education     & 0.15 & Medium \\
Checking Acc. & 0.35 & Strong \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{High-cardinality chi-square analysis.}
\begin{longtable}{lccc}
\caption{Chi-Square Test Results for High-Cardinality Categorical Features (Taiwan Dataset)}
\label{tab:chi2_taiwan}\\
	oprule
	extbf{Feature} & \textbf{Levels} & \textbf{Chi-Square} & \textbf{df} \\
\midrule
\endfirsthead
	oprule
	extbf{Feature} & \textbf{Levels} & \textbf{Chi-Square} & \textbf{df} \\
\midrule
\endhead
\midrule
\multicolumn{4}{r}{\textit{Continued on next page}} \\
\endfoot
\bottomrule
\endlastfoot
X12 & 22723 & 22538.26282 & 22722 \\
X13 & 22346 & 22220.16053 & 22345 \\
X14 & 22026 & 21948.1175  & 22025 \\
X15 & 21548 & 21490.05406 & 21547 \\
X16 & 21010 & 20941.89468 & 21009 \\
X17 & 20604 & 20539.77654 & 20603 \\
X18 & 7943  & 7109.218638 & 7942  \\
X19 & 7899  & 6700.578972 & 7898  \\
X20 & 7518  & 6636.210262 & 7517  \\
X23 & 6939  & 6188.916847 & 6938  \\
X21 & 6937  & 6077.420899 & 6936  \\
X22 & 6897  & 6074.352701 & 6896  \\
X6  & 11    & 5365.964977 & 10    \\
X7  & 11    & 3474.46679  & 10    \\
X8  & 11    & 2622.462128 & 10    \\
X9  & 11    & 2341.469945 & 10    \\
X10 & 10    & 2197.694901 & 9     \\
X11 & 10    & 1886.835309 & 9     \\
X1  & 81    & 1010.018493 & 80    \\
X3  & 7     & 163.2165579 & 6     \\
X5  & 56    & 158.5529001 & 55    \\
X2  & 2     & 47.90543312 & 1     \\
X4  & 4     & 35.66239583 & 3     \\
\end{longtable}

\noindent\textbf{Interpretation.} Features X12–X21 are extremely high-cardinality; despite large chi-square values (due to high degrees of freedom), they require transformation (grouping, frequency/WOE encoding) to avoid overfitting.

% =====================================================
% 5. Proposed Methodology
% =====================================================
\section{Proposed Methodology}
Our \textbf{Fuzzy-Monotonic LightGBM} pipeline integrates engineered behavioral features, fuzzy membership activations, and LightGBM with monotonic constraints.

\subsection{Behavioral Feature Engineering}
From the Taiwan dataset, we construct interpretable aggregates such as: BILL\_AMT\_AVG (mean of six bill statements), utilization and repayment ratios, delinquency summaries, and a payment trend (slope between first and last repayment amounts). These capture consistency, credit consumption, and repayment discipline.

\subsection{Fuzzy Membership Layer}
For selected numeric variables, we define three linguistic memberships: \emph{Low}, \emph{Medium}, \emph{High}. Cut-points are learned using robust percentiles. Example rule semantics: \emph{IF credit limit is low AND recent delinquency is high THEN default risk is high}. Rule activations (via min-\textit{t}-norm) are appended as features.

\subsection{Monotonic Gradient Boosting}
Economic priors are enforced via LightGBM monotonic constraints, e.g., higher credit limit $\Rightarrow$ lower risk; higher delinquency severity $\Rightarrow$ higher risk; older age $\Rightarrow$ lower risk. Monotonicity improves stability under drift and aligns with governance expectations.

\subsection{Explainability Layer}
Tree SHAP provides global and local attributions; combined with fuzzy rule activations and monotonicity, this yields both structural and attributional explainability.

\subsection{Overall Pipeline}
\begin{enumerate}
		\item Preprocess datasets and engineer behavioral features
		\item Compute fuzzy memberships and rule activations
		\item Train LightGBM with monotonic constraints and calibrated probabilities
		\item Evaluate, ablate, and interpret using SHAP and rule activations
\end{enumerate}

% =====================================================
% 6. Experiments and Results
% =====================================================
\section{Experiments and Results}
\subsection{Experimental Setup}
We use an 80/20 stratified split on the Taiwan dataset. Categorical variables are label-encoded; numeric variables are robust-scaled (fit on train only). Class imbalance is handled via \texttt{class\_weight='balanced'} without synthetic sampling. Metrics focus on probability quality and ranking: PR-AUC (primary), ROC-AUC, Brier score/calibration, and KS statistic.

\subsection{Model Variants}
We compare: (1) Baseline Raw (original features), (2) Baseline + Engineered, (3) Fuzzy + Engineered, (4) Fuzzy-Monotonic (proposed).

\subsection{Baseline Behavior Prior to Hybrid Model}
\begin{figure}[H]
\centering
\maybeincludegraphic[width=0.47\textwidth]{results/pr_curve.png}
\caption{Precision–Recall curves comparing Logistic Regression vs LightGBM baselines prior to fuzzy/monotonic enhancements.}
\label{fig:baseline_prcurve}
\end{figure}

\subsection{Quantitative Results}
Detailed performance and ablation results are available in the repository's results JSON artifacts (e.g., \texttt{Latex/results/ablation\_table.json}, \texttt{Latex/results/baseline\_engineered\_metrics.json}). These demonstrate that fuzzy memberships improve calibration and PR-AUC stability, while monotonic constraints preserve economic directionality with minimal loss in discrimination.

% =====================================================
% 7. Discussion and Practical Implications
% =====================================================
\section{Discussion and Practical Implications}
The hybrid approach is suitable for regulated deployment: monotonicity enforces directionally correct responses; fuzzy activations provide human-readable reasoning; SHAP complements with quantitative attributions. Together, they support challenger model adoption, pricing explainability, and IFRS-9/CECL governance documentation.

% =====================================================
% 8. Limitations and Future Work
% =====================================================
\section{Limitations and Future Work}
High-cardinality categorical variables in Taiwan require careful encoding to prevent leakage and overfitting. Future work includes: automated monotonic prior discovery; uncertainty quantification; stability under macro shocks; and conversion of fuzzy rules into compact scorecards for operations.

% =====================================================
% 9. Conclusion
% =====================================================
\section{Conclusion}
We presented a \textbf{Fuzzy-Monotonic LightGBM} framework with integrated EDA, demonstrating competitive predictive performance, improved calibration, and structural interpretability aligned with supervisory requirements. The merged manuscript consolidates methodology, experiments, and rich EDA for both German and Taiwan datasets in an Elsevier-ready format.

% =====================================================
% References (placeholder to avoid missing citations)
% =====================================================
\begin{thebibliography}{00}
\bibitem{b1} Hand, D. J., and Henley, W. E. (1997). Statistical classification methods in consumer credit scoring: a review. Journal of the Royal Statistical Society: Series A.
\bibitem{b2} Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics.
\bibitem{b3} Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence.
\bibitem{b4} Chen, T., and Guestrin, C. (2016). XGBoost: A scalable tree boosting system. KDD.
\bibitem{b6} Ben-David, E. et al. (2020). Monotonic constraint methods for tree-based models. arXiv preprint.
\bibitem{b7} Ke, G. et al. (2017). LightGBM: A highly efficient gradient boosting decision tree. NIPS.
\bibitem{b8} European Central Bank (2017). TRIM Guide to Internal Models. ECB.
\bibitem{b9} Lundberg, S., and Lee, S.-I. (2017). A unified approach to interpreting model predictions. NIPS.
\bibitem{b10} Sculley, D. et al. (2015). Hidden technical debt in machine learning systems. NIPS.
\bibitem{b11} Breiman, L. (2001). Random Forests. Machine Learning.
\end{thebibliography}

\end{document}
