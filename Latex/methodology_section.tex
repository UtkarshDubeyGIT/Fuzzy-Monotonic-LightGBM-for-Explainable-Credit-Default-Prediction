\section{Methodology}

This section details the end-to-end methodology of the Explainable Fuzzy Credit-Risk Prediction (X-FuzzyScore) framework, covering data preprocessing, fuzzy inference, machine learning models, hybridization strategies, explainability, and evaluation. We emphasize transparent and reproducible practices to ensure both predictive performance and interpretability.

\subsection{Pipeline Overview}

% filepath: /workspaces/Credit-Risk-Analysis-and-Prediction-Framework/Latex/x_fuzzyscore_pipeline_diagram.tex

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=1.75cm, auto, >=latex, thin]
        % Nodes
        \node[draw, rectangle, rounded corners, fill=blue!10, align=center] (data) {Data \\Ingestion \& \\Cleaning};
        \node[draw, rectangle, rounded corners, fill=green!10, right=of data, align=center] (fe) {Feature \\Engineering};
        \node[draw, rectangle, rounded corners, fill=orange!10, right=of fe, align=center] (fuzzy) {Fuzzy \\Inference \\Layer};
        \node[draw, rectangle, rounded corners, fill=yellow!10, below=1.5cm of fuzzy, align=center] (ml) {ML\\ Modeling};
        \node[draw, rectangle, rounded corners, fill=purple!10, right=of fuzzy, align=center] (hybrid) {Hybrid \\ Fusion};
        \node[draw, rectangle, rounded corners, fill=red!10, right=of hybrid, align=center] (explain) {Explainability \\(SHAP)};
        \node[draw, rectangle, rounded corners, fill=gray!20, below=1.5cm of explain] (eval) {Evaluation};

        % Arrows
        \draw[->] (data) -- (fe);
        \draw[->] (fe) -- (fuzzy);
        \draw[->] (fe) |- (ml);
        \draw[->] (fuzzy) -- (hybrid);
        \draw[->] (ml) -| (hybrid);
        \draw[->] (hybrid) -- (explain);
        \draw[->] (hybrid) -- (eval);
        \draw[->] (explain) -- (eval);
    \end{tikzpicture}
    \caption{X-FuzzyScore Methodology Pipeline: Data preprocessing, feature engineering, fuzzy inference, ML modeling, hybrid fusion, explainability, and evaluation.}
    \label{fig:x_fuzzyscore_pipeline}
\end{figure}

\begin{enumerate}
    \item Data ingestion and cleaning (schema alignment, recoding, deduplication).
    \item Feature engineering (winsorization/capping, scaling if required).
    \item Fuzzy inference layer to compute a human-interpretable risk score.
    \item ML modeling (Logistic Regression, Random Forest, XGBoost, LightGBM).
    \item Hybridization: fuse fuzzy and ML via feature augmentation or late fusion.
    \item Explainability: SHAP-based local and global attributions.
    \item Evaluation: metrics, threshold selection, and calibration checks.
\end{enumerate}

\subsection{Data Preprocessing}

We use the \href{https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data}{UCI Statlog German Credit Data Default} dataset as the primary benchmark.\footnote{\url{https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data}} 
% Let the raw dataset be \(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N\), where \(y_i \in \{0,1\}\) indicates non-default or default.
\paragraph{Notation.}
We consider a binary classification dataset
\[
\mathcal{D}=\{(x_i,y_i)\}_{i=1}^N,\qquad x_i\in\mathbb{R}^d,\; y_i\in\{0,1\},
\]
where \(y_i=1\) denotes default and \(y_i=0\) non-default. Let
\[
X=\begin{bmatrix}x_1^\top\\ \vdots \\ x_N^\top\end{bmatrix}\in\mathbb{R}^{N\times d},\quad
y=\begin{bmatrix}y_1\\ \vdots \\ y_N\end{bmatrix}\in\{0,1\}^N.
\]

Preprocessing steps (from the EDA notebook):
\begin{itemize}
    \item \textbf{Column normalization:} unify naming (e.g., \texttt{default payment next month} \(\rightarrow\) \texttt{default\_payment}; \texttt{PAY\_0} \(\rightarrow\) \texttt{PAY\_1}) and lowercase all columns. \emph{Rationale:} consistent naming prevents errors and improves code maintainability across team workflows.
    
    \item \textbf{Categorical recoding:} group undocumented categories into ``Others'' (\texttt{education} \(0,5,6\) \(\rightarrow\) 4; \texttt{marriage} \(0\) \(\rightarrow\) 3). \emph{Rationale:} sparse or undefined categories cause instability in tree splits and one-hot encodings; consolidation improves robustness and prevents overfitting to noise.
    
    \item \textbf{Payment status normalization:} map \(-2,-1\) to \(0\) in \texttt{pay\_1}--\texttt{pay\_6} to represent no-consumption or paid-in-full as non-delinquency. \emph{Rationale:} original codes \(-2\) (no consumption) and \(-1\) (paid in full) both indicate good standing and should not be treated as missing or distinct risk categories.
    
    \item \textbf{Deduplication:} drop duplicates after normalization to avoid leakage and inflated support. \emph{Rationale:} duplicate rows artificially inflate training counts and can lead to overfitting or biased performance metrics.
    
    \item \textbf{Outlier handling (Winsorization):} for heavy-tailed features (\texttt{limit\_bal}, \texttt{bill\_amt\*}, \texttt{pay\_amt\*}), apply percentile-based capping at the 1st and 99th quantiles:
    \begin{equation}
        x_i^{\text{capped}} = \begin{cases}
            p_1 & \text{if } x_i < p_1, \\
            x_i & \text{if } p_1 \le x_i \le p_{99}, \\
            p_{99} & \text{if } x_i > p_{99},
        \end{cases}
    \end{equation}
    where \(p_1 = \text{quantile}(x, 0.01)\) and \(p_{99} = \text{quantile}(x, 0.99)\). This obtains features like \texttt{limit\_bal\_capped}, \texttt{bill\_amt1\_capped}, etc.
    
    \emph{Rationale:} right-skewed distributions (observed in EDA histograms/boxplots) with extreme outliers distort tree-based partitioning and inflate variance estimates. Winsorization reduces skewness (e.g., from \(\approx 5.0\) to \(\approx 0.07\) for \texttt{bill\_amt1}) while preserving rank order and avoiding artificial zero-inflation (unlike median imputation) or negative skew (unlike log transforms). Empirical comparison shows winsorization outperforms: (a) \emph{median replacement}, which creates artificial spikes and loses distributional structure, and (b) \emph{log transformation}, which over-corrects and produces negative skew, especially when data contains zeros.
\end{itemize}

We use an 80/20 train-test split (\(n_{train}=23971\), \(n_{test}=5993\)) with stratification to preserve class proportions. Class imbalance is addressed during evaluation via threshold tuning and by optionally using class-weighted losses in ML models.

\subsection{Fuzzy Inference Layer}

\paragraph{Purpose and motivation.} The fuzzy layer provides a human-interpretable risk score \(s_{\text{fuzzy}} \in [0,1]\) by encoding domain knowledge through linguistic rules. Unlike black-box ML models, fuzzy systems express risk assessment in natural language (e.g., ``IF credit limit is low AND payment status is poor THEN risk is high''), making them auditable and explainable to regulators and stakeholders.

We define \emph{linguistic variables} over selected features (e.g., credit limit, bill amounts, delinquency, payment ratios), each with \emph{membership functions} for labels such as Low, Medium, and High. Each membership function \(\mu_\ell:\mathbb{R}\to[0,1]\) quantifies the degree to which a feature value belongs to the linguistic category \(\ell\).

\subsubsection{Membership Functions}

\paragraph{Triangular membership functions.} For a scalar feature \(x\), a triangular membership function for label \(\ell\) is defined as:
\begin{equation}
    \mu_{\ell}(x; a,b,c) = \max\left(\min\left(\frac{x-a}{b-a}, \frac{c-x}{c-b}\right), 0\right),\; a < b < c,
\end{equation}
where \(a\), \(b\), \(c\) are the left base, peak, and right base respectively. This piecewise-linear function rises from 0 at \(a\) to 1 at \(b\), then falls back to 0 at \(c\).

\paragraph{Trapezoidal membership functions.} Trapezoidal sets \(\mu_{\ell}(x; a,b,c,d)\) generalize triangular functions by allowing a plateau (full membership) between \(b\) and \(c\):
\begin{equation}
    \mu_{\ell}(x; a,b,c,d) = \max\left(\min\left(\frac{x-a}{b-a}, 1, \frac{d-x}{d-c}\right), 0\right),\; a < b \le c < d.
\end{equation}

\paragraph{Parameter selection.} Parameters \((a,b,c,d)\) are chosen from quantiles of the training data (e.g., 20th, 50th, 80th percentiles) or domain guidelines. For instance, if ``Low credit limit'' is defined by the bottom 40\% of observed values, we set \(a=\min(x)\), \(b=c=p_{40}\), and use trapezoidal or triangular shapes as appropriate.

\emph{Rationale:} quantile-based parameters adapt to data scale and distribution, ensuring membership functions remain meaningful across datasets. This data-driven approach balances expert knowledge with empirical evidence.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figures/germany_008.png}
        \caption{Credit Limit membership functions (Low, Medium, High)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figures/germany_009.png}
        \caption{Payment Status membership functions (Good, Fair, Poor)}
    \end{subfigure}
    
    \vspace{0.3cm}
    
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figures/germany_010.png}
        \caption{Bill Amount Ratio membership functions}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figures/germany_011.png}
        \caption{Age membership functions (Young, Middle, Senior)}
    \end{subfigure}
    \caption{Triangular membership functions for fuzzy antecedent variables. Each linguistic label (e.g., Low, Medium, High) is represented by a triangular function with parameters derived from data quantiles. These functions map normalized feature values to degrees of membership in \([0,1]\).}
    \label{fig:fuzzy_membership_antecedents}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{./figures/germany_012.png}
    \caption{Risk Score (consequent) membership functions with three linguistic categories: Low risk (0--40), Medium risk (30--70), and High risk (60--100). The overlapping regions allow smooth transitions between risk levels during defuzzification.}
    \label{fig:fuzzy_membership_consequent}
\end{figure}

\subsubsection{Rule Base and Inference}

\paragraph{Mamdani inference system.} We use Mamdani inference with min (\(\wedge\)) as the t-norm (AND operator) and max (\(\vee\)) as the s-norm (OR operator). The system encodes 8 domain-knowledge-based rules derived from credit risk heuristics. Example rules:
\begin{align}
    &\textbf{R1:}\; \text{IF } \text{credit\_limit is Low} \wedge \text{payment\_status is Poor} \Rightarrow \text{risk is High},\\
    &\textbf{R2:}\; \text{IF } \text{credit\_limit is Low} \wedge \text{bill\_ratio is High} \Rightarrow \text{risk is High},\\
    &\textbf{R3:}\; \text{IF } \text{payment\_status is Poor} \wedge \text{bill\_ratio is High} \Rightarrow \text{risk is High},\\
    &\textbf{R4:}\; \text{IF } \text{credit\_limit is High} \wedge \text{payment\_status is Good} \Rightarrow \text{risk is Low},\\
    &\textbf{R5:}\; \text{IF } \text{credit\_limit is Medium} \wedge \text{payment\_status is Fair} \Rightarrow \text{risk is Medium},\\
    &\textbf{R6:}\; \text{IF } \text{age is Young} \wedge \text{payment\_status is Poor} \Rightarrow \text{risk is High},\\
    &\textbf{R7:}\; \text{IF } \text{age is Senior} \wedge \text{payment\_status is Good} \Rightarrow \text{risk is Low},\\
    &\textbf{R8:}\; \text{IF } \text{bill\_ratio is Low} \wedge \text{payment\_status is Good} \Rightarrow \text{risk is Low}.
\end{align}

\paragraph{Inference mechanics.} For each rule, the antecedent membership \(\alpha\) is computed via the t-norm (minimum) of individual memberships:
\begin{equation}
    \alpha = \min(\mu_{\text{A1}}(x_1), \mu_{\text{A2}}(x_2), \ldots),
\end{equation}
where \(\mu_{\text{Ai}}\) are the membership functions for the antecedent conditions. The implied consequent fuzzy set is clipped at height \(\alpha\):
\begin{equation}
    \mu_{C'}(z) = \min(\alpha, \mu_C(z)),
\end{equation}
where \(\mu_C(z)\) is the consequent membership function (e.g., risk is High). Aggregation across all triggered rules uses the s-norm (maximum):
\begin{equation}
    \mu_{\text{agg}}(z) = \max(\mu_{C'_1}(z), \mu_{C'_2}(z), \ldots, \mu_{C'_R}(z)).
\end{equation}

\paragraph{Defuzzification.} The crisp output risk score \(s_{\text{fuzzy}}\) is obtained via centroid (center of gravity) defuzzification:
\begin{equation}
    s_{\text{fuzzy}} = \frac{\int z\, \mu_{\text{agg}}(z)\,dz}{\int \mu_{\text{agg}}(z)\,dz},\quad z \in [0,100],
\end{equation}
where the integral is computed over the output universe (0–100 risk score scale). This is then normalized to \([0,1]\) for model integration.

\emph{Rationale:} Mamdani inference is intuitive and widely used in fuzzy control; the centroid method balances all activated rules' contributions. The min/max operators are standard choices providing robust, monotonic aggregation without requiring calibration of weight parameters.

\paragraph{Fuzzy inference example.} To illustrate the inference process, consider a sample customer with:
\begin{itemize}
    \item Normalized credit limit: 0.25 (low-medium range)
    \item Normalized payment status: 0.80 (poor payment history)
    \item Normalized bill ratio: 0.70 (high utilization)
    \item Normalized age: 0.35 (young)
\end{itemize}

Step-by-step evaluation:
\begin{enumerate}
    \item \textbf{Fuzzification:} Compute membership degrees:
    \begin{align*}
        \mu_{\text{credit\_limit,low}}(0.25) &= 0.60,\quad \mu_{\text{credit\_limit,medium}}(0.25) = 0.40,\\
        \mu_{\text{payment\_status,poor}}(0.80) &= 0.67,\quad \mu_{\text{bill\_ratio,high}}(0.70) = 0.50,\\
        \mu_{\text{age,young}}(0.35) &= 0.75.
    \end{align*}
    
    \item \textbf{Rule activation:} Evaluate applicable rules using min (AND):
    \begin{align*}
        \alpha_{\text{R1}} &= \min(0.60, 0.67) = 0.60\quad \text{(credit\_limit low AND payment\_status poor)},\\
        \alpha_{\text{R3}} &= \min(0.67, 0.50) = 0.50\quad \text{(payment\_status poor AND bill\_ratio high)},\\
        \alpha_{\text{R6}} &= \min(0.75, 0.67) = 0.67\quad \text{(age young AND payment\_status poor)}.
    \end{align*}
    All three rules point to ``risk high'' consequent.
    
    \item \textbf{Aggregation:} Clip and aggregate consequent sets:
    \[
    \mu_{\text{risk,agg}}(z) = \max(0.60 \cdot \mu_{\text{high}}(z), 0.50 \cdot \mu_{\text{high}}(z), 0.67 \cdot \mu_{\text{high}}(z)) = 0.67 \cdot \mu_{\text{high}}(z).
    \]
    
    \item \textbf{Defuzzification:} Compute centroid of clipped ``high'' membership function:
    \[
    s_{\text{fuzzy}} = \frac{\int z \cdot \min(0.67, \mu_{\text{high}}(z))\,dz}{\int \min(0.67, \mu_{\text{high}}(z))\,dz} \approx 76.4 \rightarrow 0.764\;\text{(normalized)}.
    \]
\end{enumerate}

This yields a high fuzzy risk score (0.76), consistent with poor payment history, high bill ratio, and young age—all red flags for credit default risk.

\paragraph{Fuzzy Risk Score Distribution.} After defuzzification, the fuzzy layer produces a continuous risk score for each sample. Figure~\ref{fig:fuzzy_risk_distribution} shows the distribution of computed fuzzy scores across the training set. The scores effectively discriminate between default and non-default cases, with defaulters exhibiting higher median risk scores. This validates that the rule-based fuzzy system captures meaningful risk patterns even before ML integration.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{./figures/germany_014.png}
    \caption{Distribution of fuzzy risk scores (left: histogram with mean line; right: boxplot stratified by default status). Defaulters show significantly higher fuzzy risk scores than non-defaulters, demonstrating the discriminative power of the rule-based fuzzy inference system.}
    \label{fig:fuzzy_risk_distribution}
\end{figure}

\subsection{Machine Learning Models}

We train standard classifiers on engineered features (23 baseline features) and on the hybrid set that appends \(s_{\text{fuzzy}}\) (24 features). Let \(x \in \mathbb{R}^d\), \(y \in \{0,1\}\).

\paragraph{Logistic Regression (LR).} A linear classifier predicting default probability via the logistic (sigmoid) function:
\begin{equation}
    p(y=1\mid x) = \sigma(w^\top x + b) = \frac{1}{1 + e^{-(w^\top x + b)}},
\end{equation}
where \(w\in\mathbb{R}^d\) are feature weights and \(b\in\mathbb{R}\) is the intercept. Parameters are learned by minimizing the binary cross-entropy loss with L2 regularization:
\begin{equation}
    \mathcal{L}(w,b) = -\sum_{i=1}^N \left[y_i \log p_i + (1-y_i)\log(1-p_i)\right] + \lambda \|w\|_2^2,
\end{equation}
where \(\lambda>0\) controls regularization strength. We use class-weighted loss (\texttt{class\_weight='balanced'}) to address imbalance.

\emph{Why use LR:} provides interpretable coefficients, fast training, and serves as a linear baseline. Regularization prevents overfitting on correlated features.

\paragraph{Random Forest (RF).} An ensemble of \(T\) decision trees \(\{h_t\}_{t=1}^T\), each trained on a bootstrap sample with random feature subsampling at each split. Prediction aggregates tree posteriors:
\begin{equation}
    p(y=1\mid x) = \frac{1}{T}\sum_{t=1}^T h_t(x).
\end{equation}
We use \(T=100\) trees with max depth 10, \texttt{max\_features='sqrt'}, and class weighting to balance sensitivity.

\emph{Why use RF:} robust to outliers, handles non-linearities naturally, and provides feature importances. Randomness reduces overfitting and variance.

\paragraph{Gradient Boosting (XGBoost/LightGBM).} Optimizes an additive tree ensemble \(F_M(x) = \sum_{m=1}^M \gamma_m h_m(x)\) via gradient descent on a regularized objective:
\begin{equation}
    \mathcal{L} = \sum_{i=1}^N \ell(y_i, F_M(x_i)) + \sum_{m=1}^M \Omega(h_m),
\end{equation}
where \(\ell\) is logistic loss and \(\Omega(h_m)\) penalizes tree complexity (number of leaves, L2 norm of leaf weights). Both XGBoost and LightGBM use histogram-based splits for efficiency and support scale\_pos\_weight for imbalance correction.

\emph{Why use gradient boosting:} state-of-the-art performance on tabular data, sequential correction of residuals improves accuracy. XGBoost offers regularization; LightGBM is faster via leaf-wise growth and gradient-based one-side sampling (GOSS).

\subsection{Hybridization Strategies}

We combine fuzzy and ML predictions in two complementary ways:
\begin{description}
    \item[Feature Augmentation:] concatenate the fuzzy score to the feature vector: \(x' = [x; s_{\text{fuzzy}}]\). The classifier learns to weight \(s_{\text{fuzzy}}\) with other features.
    \item[Late Fusion:] combine calibrated posteriors: \(\hat{p} = (1-\lambda)\, p_{\text{ML}} + \lambda\, s_{\text{fuzzy}}\), \(\lambda \in [0,1]\) tuned on validation data, or learn a meta-learner \(g\big(p_{\text{ML}}, s_{\text{fuzzy}}\big)\).
\end{description}

\subsection{Explainability with SHAP}

We adopt SHAP (SHapley Additive exPlanations), a unified framework for interpreting model predictions based on game-theoretic Shapley values. SHAP provides explanations of the form:
\begin{equation}
    f(x) \approx \phi_0 + \sum_{i=1}^d \phi_i,
\end{equation}
where \(\phi_0\) is the base value (expected model output over the training set) and \(\phi_i\) is the Shapley value for feature \(i\), representing its marginal contribution to the prediction. Shapley values are computed from coalitional game theory:
\begin{equation}
    \phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(d-|S|-1)!}{d!} \left[f_S(x_{S \cup \{i\}}) - f_S(x_S)\right],
\end{equation}
where \(S\) are all possible feature subsets excluding \(i\), and \(f_S\) is the model prediction conditioned on feature subset \(S\).

\paragraph{Implementation.} For tree-based models (Random Forest, XGBoost, LightGBM), we use TreeSHAP, an efficient polynomial-time algorithm that exploits tree structure to compute exact Shapley values. For linear models like Logistic Regression, SHAP values reduce to scaled coefficients: \(\phi_i \propto w_i \cdot (x_i - \mathbb{E}[x_i])\).

\paragraph{Global and local interpretability.} We report:
\begin{itemize}
    \item \textbf{Global feature importance:} mean absolute SHAP value \(\text{mean}(|\phi_i|)\) across all samples, indicating average impact magnitude.
    \item \textbf{Local explanations:} SHAP force plots and waterfall diagrams for individual predictions, especially for borderline cases near the decision threshold (\(\hat{p} \approx 0.5\)).
\end{itemize}

\emph{Rationale:} SHAP satisfies three desirable properties: \emph{local accuracy} (explanations sum to the prediction), \emph{missingness} (zero-valued features have zero attribution), and \emph{consistency} (if a model changes to increase a feature's impact, its SHAP value should not decrease). This theoretical foundation makes SHAP more reliable than alternative methods like LIME for feature attribution.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{./figures/germany_015.png}
    \caption{SHAP summary plot showing global feature importance (left: bar plot of mean absolute SHAP values) and feature impact patterns (right: beeswarm plot where each point is a sample, colored by feature value). Features like payment status, credit limit, and bill amounts emerge as top contributors to risk prediction. The fuzzy risk score integrates well, ranking among important features in hybrid models.}
    \label{fig:shap_summary}
\end{figure}

\paragraph{Fuzzy-SHAP integration.} In hybrid models, the \texttt{fuzzy\_risk\_score} feature receives its own SHAP value, quantifying how much the interpretable fuzzy layer contributes to final predictions relative to raw features. This creates a two-level explanation hierarchy: (1) SHAP explains the hybrid model's reliance on \(s_{\text{fuzzy}}\) versus other features, and (2) the fuzzy rule base explains how \(s_{\text{fuzzy}}\) itself was computed from linguistic rules. This dual-layer interpretability is valuable for regulatory compliance and stakeholder communication.

\subsection{Representative EDA Visuals}

We include representative plots extracted from the EDA notebook to ground the methodology and preprocessing choices:

% \begin{figure}[h]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./figures/process1_img_005.png}
%         \caption{Default vs non-default distribution}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./figures/process1_img_006.png}
%         \caption{Education level distribution}
%     \end{subfigure}
%     \caption{Target imbalance and categorical distribution motivate threshold tuning and careful encoding.}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./figures/process1_img_007.png}
%         \caption{Credit limit histogram}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./figures/process1_img_008.png}
%         \caption{Credit limit boxplot}
%     \end{subfigure}
%     \caption{Right-skew and outliers motivate winsorization/capping strategies.}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{./figures/process1_img_009.png}
%     \caption{Correlation heatmap indicating strong temporal correlations among bill amount features.}
% \end{figure}

\subsection{Evaluation Protocol}

\paragraph{Split and Metrics} We evaluate on a held-out test set (\(20\%\)). Metrics include Accuracy, Precision, Recall, F1, and ROC-AUC:
\begin{align}
    \text{Precision} &= \frac{TP}{TP + FP},\quad \text{Recall} = \frac{TP}{TP + FN},\\
    \text{F1} &= 2\,\frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}},\quad \text{AUC} = \int_0^1 \text{TPR}(\text{FPR})\,d\,\text{FPR}.
\end{align}

\paragraph{Thresholding and Imbalance} Because defaults are the minority class, we tune the decision threshold \(\tau\) to optimize F1 or Youden's \(J = \text{TPR} - \text{FPR}\). We also consider class-weighted losses and probability calibration checks (e.g., reliability curves).

\paragraph{Reported Results} On the Taiwan dataset, the best-performing hybrids achieve ROC-AUC around 0.77 with Recall around 0.60 (e.g., LightGBM+Fuzzy and XGBoost+Fuzzy), while Random Forest baselines attain similar AUC but slightly different precision-recall trade-offs.

\subsection{Reproducibility}

We fix random seeds, log preprocessing parameters (e.g., capping percentiles), and record feature schemas. The hybrid feature set includes the fuzzy score, yielding 24 features total, with baseline features numbering 23. Train/test sizes are fixed at \(23971/5993\) for comparability.

\subsection{Computational Considerations}

Tree ensembles scale approximately with \(\mathcal{O}(M\, d\, n \log n)\) for \(M\) trees and \(n\) samples. Fuzzy inference is linear in the number of rules and variables. The overhead of SHAP (TreeSHAP) is polynomial in tree depth and tree count but efficient for tree models.

\subsection{Method Summary}

The X-FuzzyScore methodology achieves \emph{multi-level interpretability} through strategic integration of fuzzy logic and explainable AI:

\paragraph{Layer 1: Fuzzy Rule-Based Transparency.} The fuzzy inference system provides inherent interpretability via:
\begin{itemize}
    \item \textbf{Linguistic rules} expressing domain knowledge in natural language (e.g., ``IF credit limit is low AND payment status is poor THEN risk is high'')
    \item \textbf{Visual membership functions} (Figures~\ref{fig:fuzzy_membership_antecedents}--\ref{fig:fuzzy_membership_consequent}) showing how feature values map to linguistic labels
    \item \textbf{Transparent inference} with explicit fuzzification, rule activation, aggregation, and defuzzification steps
    \item \textbf{Auditable scores} where any prediction can be traced back through activated rules to understand \emph{why} a risk score was assigned
\end{itemize}

\paragraph{Layer 2: SHAP Post-Hoc Explanations.} For the hybrid ML models, SHAP analysis (Figure~\ref{fig:shap_summary}) provides:
\begin{itemize}
    \item \textbf{Global feature importance} identifying which features (including \texttt{fuzzy\_risk\_score}) drive overall model behavior
    \item \textbf{Local explanations} for individual predictions, showing feature-level contributions
    \item \textbf{Validation of fuzzy integration} by quantifying how much the interpretable fuzzy layer contributes versus raw features
\end{itemize}

\paragraph{Hybrid advantage.} The combination delivers:
\begin{enumerate}
    \item \textbf{Improved recall:} fuzzy scores help identify high-risk cases that pure ML might miss, boosting sensitivity to defaults
    \item \textbf{Maintained accuracy:} competitive ROC-AUC (~0.77) shows no significant performance sacrifice for interpretability
    \item \textbf{Regulatory compliance:} dual-layer explanations satisfy both human understanding (fuzzy rules) and technical rigor (SHAP values)
    \item \textbf{Stakeholder trust:} credit officers can understand risk assessments through linguistic rules, while data scientists can validate via SHAP
\end{enumerate}

This architecture bridges the gap between black-box ML performance and transparent, explainable decision-making required in high-stakes financial applications.

\subsection{Training Configuration and Hyperparameters}

Unless otherwise specified, models are trained with the following default settings (tuned via validation or simple grid search):
\begin{itemize}
    \item Logistic Regression: penalty=L2, $C \in \{0.1, 1, 10\}$, class\_weight=\texttt{balanced} (optional).
    \item Random Forest: $n$\_estimators $\in [200, 600]$, max\_depth $\in \{None, 8, 12\}$, max\_features=sqrt.
    \item XGBoost/LightGBM: learning\_rate $\in [0.03, 0.1]$, n\_estimators $\in [300, 800]$, max\_depth $\in \{4,6,8\}$, subsample/colsample\_by\_tree $\in [0.6, 0.9]$.
    \item Fuzzy layer: membership parameters derived from quantiles (e.g., 20/50/80th), rule base curated to cover high-risk and low-risk archetypes.
\end{itemize}

Feature scaling is not mandatory for tree models; for LR, we use either standardization or leave raw if features are capped and numerically stable. Categorical features (e.g., \texttt{sex}, \texttt{education}, \texttt{marriage}) are kept as integers, consistent with domain semantics.

\subsection{Leakage Prevention and Validation}

All preprocessing (including capping percentiles and membership calibration) is performed within the training data scope and applied to the test set using frozen parameters to prevent leakage. Validation strategies considered:
\begin{itemize}
    \item Hold-out: 80/20 split as reported.
    \item 5-fold cross-validation for hyperparameter tuning; final metrics reported on the hold-out test.
    \item Temporal awareness: although the dataset is static, the payment history order is preserved; no target leakage features are introduced.
\end{itemize}

\subsection{Calibration and Threshold Selection}

We optionally apply Platt scaling or isotonic regression to calibrate tree ensemble probabilities. Threshold \(\tau\) is selected to maximize F1 or based on application utility (e.g., cost-sensitive thresholding):
\begin{equation}
    	au^* = \arg\max_{\tau} \; U(\text{TP}(\tau), \text{FP}(\tau), \text{FN}(\tau)),
\end{equation}
where $U$ encodes operational costs/benefits.

\subsection{Uncertainty and Statistical Testing}

We compute 95\% confidence intervals via stratified bootstrap (1{,}000 resamples) for AUC and F1. For model comparisons, we report paired bootstrap $p$-values or DeLong's test for ROC-AUC when applicable.

\subsection{Algorithmic Summaries}

\paragraph{Fuzzy Inference (Mamdani)}
\begin{enumerate}
    \item Compute antecedent memberships for selected features.
    \item Evaluate each rule with t-norm (min) to get activation $\alpha$.
    \item Clip consequent set by $\alpha$ and aggregate via s-norm (max).
    \item Defuzzify aggregated set by centroid to obtain $s_{\text{fuzzy}}$.
\end{enumerate}

\paragraph{Hybrid Late Fusion}
\begin{enumerate}
    \item Train ML model to obtain $p_{\text{ML}}(x)$.
    \item Compute $s_{\text{fuzzy}}(x)$ from fuzzy layer.
    \item Combine $\hat{p}(x) = (1-\lambda)\, p_{\text{ML}}(x) + \lambda\, s_{\text{fuzzy}}(x)$; tune $\lambda$ on validation.
\end{enumerate}

\subsection{Feature Overview}

Table~\ref{tab:feat} summarizes the primary engineered features; the hybrid setting appends the fuzzy risk score.
\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        	
        	\textbf{Feature group} & \textbf{Examples} \\
        \midrule
        Demographics & \texttt{sex}, \texttt{education}, \texttt{marriage}, \texttt{age} \\
        Credit limit & \texttt{limit\_bal\_capped} \\
        Payment status & \texttt{pay\_1} -- \texttt{pay\_6} (normalized) \\
        Bill amounts & \texttt{bill\_amt1\_capped} -- \texttt{bill\_amt6\_capped} \\
        Payment amounts & \texttt{pay\_amt1\_capped} -- \texttt{pay\_amt6\_capped} \\
        Hybrid addition & \texttt{fuzzy\_risk\_score} \\
        \bottomrule
    \end{tabular}
    \caption{Feature overview (baseline: 23 features; hybrid: +1 fuzzy risk score).}
    \label{tab:feat}
\end{table}

