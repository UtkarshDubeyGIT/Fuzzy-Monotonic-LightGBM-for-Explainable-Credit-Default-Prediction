\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue, citecolor=blue}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Graceful image inclusion: include placeholder box if file is missing
\newcommand{\maybeincludegraphic}[2][]{%
  \IfFileExists{#2}{\includegraphics[#1]{#2}}{\fbox{\scriptsize Missing: #2}}}

\begin{document}

\title{Fuzzy-Monotonic LightGBM for Explainable Credit Default Prediction:\\
An Extended Analysis with Comprehensive Exploratory Data Analysis}

\author{
\IEEEauthorblockN{Utkarsh Dubey}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Netaji Subhas University of Technology},\\ New Delhi, India \\
\texttt{utkarsh.dubey.ug23@nsut.ac.in}}
\and
\IEEEauthorblockN{Kanav Singla}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Netaji Subhas University of Technology}, \\ New Delhi, India \\
\texttt{kanav.singla.ug23@nsut.ac.in}}
\and
\IEEEauthorblockN{Dushyant Bhardwaj}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Netaji Subhas University of Technology}, \\ New Delhi, India \\
\texttt{dushyant.bhardwaj.ug23@nsut.ac.in}}
}

\maketitle

\begin{abstract}
Credit default prediction is challenging due to severe class imbalance, highly non-linear behavioral drivers, and regulatory requirements for transparency in high-stakes lending decisions. Modern boosted models (e.g., LightGBM) achieve strong performance but remain black-box in nature, while purely rule-based fuzzy systems are interpretable but weak at scale. This work proposes a hybrid \textbf{Fuzzy-Monotonic LightGBM} framework that combines domain-engineered financial indicators, fuzzy linguistic rule activations, and economically consistent monotonic constraints inside a unified gradient-boosted architecture. Experiments conducted on two standard benchmarks --- the Taiwan dataset (predictive evaluation) and the German dataset (interpretability demonstration) --- show that this hybrid model improves probability calibration, stabilizes PR-AUC behavior, and maintains directionally correct economic structure while preserving competitive discrimination performance. This extended version includes comprehensive exploratory data analysis (EDA) for both datasets, providing detailed insights into data quality, feature distributions, risk patterns, and statistical associations that inform our modeling choices. The results suggest that combining fuzzy reasoning with monotonic gradient boosting provides a regulator-aligned, practically deployable XAI framework for credit default prediction in real financial environments.
\end{abstract}

\begin{IEEEkeywords}
credit risk scoring, fuzzy reasoning, monotonic models, explainable AI, LightGBM, calibration, credit default prediction, exploratory data analysis
\end{IEEEkeywords}

% =====================================================
% 1. INTRODUCTION
% =====================================================
\section{Introduction}
Credit risk modeling is a fundamental component of retail banking, particularly in consumer lending portfolios such as credit cards, where borrower behavior displays non-linear interactions and strong macro sensitivity. Financial institutions increasingly depend on data-driven risk scoring for loan approval, pricing, capital allocation, IFRS-9 staging, and early warning systems. However, modern regulatory regimes (Basel II/III, ECB TRIM, SR-11-7) require that credit risk models must not only be accurate, but \emph{explainable}, economically interpretable, and operationally trustworthy.

This creates a persistent conflict: high-performing machine learning models (e.g., gradient boosting) significantly outperform linear risk scorecards, but are often rejected by model risk governance due to opacity, lack of directional guarantees, and difficulty in providing actionable reasoning to credit officers. The emergence of \textbf{Explainable AI (XAI)} offers a pathway to resolve this tension---but most prior work either relies solely on post-hoc attribution (e.g., SHAP) without structural interpretability, or sacrifices accuracy to remain rule-based and manually interpretable.

In financial risk modeling, interpretability is not optional---it is a supervisory requirement, and a key requirement for safe deployment.

To address this, we propose a hybrid \textbf{Fuzzy-Monotonic LightGBM} framework for credit default prediction. The approach integrates three complementary components:

\begin{itemize}
    \item engineered behavioral credit features from domain priors,
    \item fuzzy linguistic membership functions and rule activations that enable human-interpretable reasoning,
    \item monotonic-constrained gradient boosted decision trees (LightGBM) that enforce economist-aligned directional constraints.
\end{itemize}

This construction yields a model that is both empirically strong and structurally interpretable by design.

We evaluate on two standard benchmark datasets: \textbf{Taiwan Credit Card Default} (primary predictive evaluation) and \textbf{German Credit} (interpretability demonstration). A controlled ablation (baseline raw, engineered baseline, fuzzy-only, fuzzy + monotonic) shows that combining fuzzy membership activations with monotonic boosting improves calibration and economic consistency while retaining competitive predictive performance. The proposed hybrid approach, therefore, represents a practically deployable and regulator-aligned XAI framework for credit risk modeling.

This extended version presents a comprehensive exploratory data analysis for both datasets, examining data quality, feature distributions, correlations, and statistical associations. These insights directly inform our feature engineering, fuzzy membership design, and monotonic constraint selection.

% =====================================================
% 2. LITERATURE REVIEW
% =====================================================
\section{Literature Review}

\subsection{Traditional Credit Scoring Models}
Classical credit scoring historically relied on logistic regression and statistical scorecard based modeling due to high interpretability and regulatory acceptance \cite{b1}. These models assume linear relationships, limited interaction effects, and are typically hand engineered with coarse expert-selected risk indicators. While stable, such models struggle to represent non-linear borrower behavioral patterns and are increasingly insufficient for modern high-volume retail credit portfolios.

\subsection{Machine Learning and Explainability in Credit Risk}
Recent work has explored machine learning-based credit risk models such as Random Forests, SVMs, and Gradient Boosted Trees \cite{b2,b7,b11}. ML models consistently outperform linear baselines; however, they are often rejected in regulated banking production due to opacity and limited supervisory trustworthiness \cite{b3,b8}. Post-hoc attribution methods (e.g., SHAP) improved explainability but remain non-structural and can still violate economic intuition \cite{b9}. Research on monotonic constraints \cite{b4,b6,b10} attempts to inject prior domain knowledge to improve economic consistency, but these methods do not inherently provide human-understandable reasoning pathways. Parallel literature on fuzzy expert systems is highly interpretable but lacks predictive strength at scale.

\subsection{Identified Research Gap}
Existing literature therefore exposes a three-sided gap: 
(1) statistical scorecards are interpretable but weak, 
(2) ML models are strong but non-transparent, 
(3) pure fuzzy systems are interpretable but underfit. 
A unified approach that provides the performance of modern ML while preserving expert reasoning structure is missing. This motivates the development of a hybrid \textbf{fuzzy + monotonic} boosting framework that is simultaneously performant, regulator-aligned, and structurally interpretable.

% =====================================================
% 3. DATASET DESCRIPTION
% =====================================================
\section{Dataset Description}

Two publicly available benchmark datasets were used in this study. The Taiwan dataset served as the primary dataset for predictive model development and ablation experimentation. The German dataset was used as a secondary dataset primarily to demonstrate interpretability and rule transparency.

\subsection{Taiwan Credit Card Default Dataset}
The Taiwan credit card default dataset (Yeh \& Lien, 2009) consists of 30,000 individual credit card clients with monthly billing and repayment behavior recorded for six consecutive months. The target variable is a binary indicator specifying whether the client defaulted in the following month. The dataset includes demographic attributes (age, education, marriage status), credit limit, repayment history indicators (\textit{PAY\_0..PAY\_6}), bill statements (\textit{BILL\_AMT1..BILL\_AMT6}), repayment amounts (\textit{PAY\_AMT1..PAY\_AMT6}), and other behavioral credit indicators. This dataset is widely used in academic credit scoring research due to its scale, temporal structure, and realistic consumer lending profile.

\subsection{German Credit Dataset}
The German credit dataset (Statlog) contains 1,000 applicants with categorical attributes such as housing type, saving accounts, checking accounts, purpose of loan, occupation category, and numeric attributes including age, credit amount, and loan duration. The target variable indicates whether the customer is classified as ``good'' or ``bad'' credit risk. Although smaller and coarser than the Taiwan dataset, the German dataset remains valuable due to its high interpretability and suitability for illustrating rule-based reasoning.

\subsection{Dataset Roles in This Work}
The experimental design intentionally separates predictive evaluation and interpretability demonstration as follows:

\begin{itemize}
    \item \textbf{Primary Modelling \& Ablation:} Taiwan dataset (30,000 samples)
    \item \textbf{Interpretability Demonstration:} German dataset (1,000 samples)
\end{itemize}

This structure reflects real-world credit risk modeling pipelines where large-scale behavioral datasets are used for model training and optimization, while smaller interpretable datasets are used for internal model validation, expert rule review, and risk governance documentation.

% =====================================================
% 4. EXPLORATORY DATA ANALYSIS
% =====================================================
\section{Exploratory Data Analysis}

This section summarizes the key exploratory data analysis (EDA) steps performed on both datasets. The objective is to examine data quality, detect patterns and relationships, and extract early insights that influence model development, preprocessing, and feature engineering.

\subsection{German Credit Dataset}

\subsubsection{Target Distribution}
Understanding the target class distribution provides initial insight into class imbalance, which influences model evaluation metrics and resampling strategy.

\begin{figure}[H]
  \centering
  \maybeincludegraphic[width=0.45\textwidth]{latex_files/01_risk_distribution.png}
  \caption{Risk distribution (German dataset)}
  \label{fig:german_risk_dist}
\end{figure}

The dataset shows a moderate imbalance, with ``Good'' borrowers being the majority. This suggests the need for balanced evaluation metrics such as F1-score and AUC, rather than accuracy alone.

\subsubsection{Missing Values Analysis}
Missing data analysis helps assess data quality and the need for imputation.

\begin{figure}[H]
  \centering
  \maybeincludegraphic[width=0.48\textwidth]{latex_files/00_missing_values_heatmap.png}
  \caption{Missing values percentage (German dataset)}
  \label{fig:german_missing}
\end{figure}

Most variables exhibit low missingness, except ``Checking account'' and ``Saving accounts''. These fields may require targeted imputation strategies or the use of missing indicators to preserve signal.

\subsubsection{Numeric Feature Distributions}
Understanding the distribution of numerical features aids in selecting appropriate scaling or transformations.

\begin{figure}[H]
  \centering
  \maybeincludegraphic[width=0.31\textwidth]{latex_files/hist_Age.png}
  \maybeincludegraphic[width=0.31\textwidth]{latex_files/hist_Credit amount.png}
  \maybeincludegraphic[width=0.31\textwidth]{latex_files/hist_Duration.png}
  \caption{Histograms of Age, Credit amount, and Duration (German dataset)}
  \label{fig:german_histograms}
\end{figure}

The variables show right-skewed distributions, especially for \textit{Credit amount} and \textit{Duration}, suggesting that log transformation may help models sensitive to scale, such as logistic regression.

\subsubsection{Numeric Features vs. Target}
Boxplots illustrate whether numerical features differentiate between defaulting vs. non-defaulting customers.

\begin{figure}[H]
  \centering
  \maybeincludegraphic[width=0.31\textwidth]{latex_files/num_box_Age_by_risk.png}
  \maybeincludegraphic[width=0.31\textwidth]{latex_files/num_box_Credit amount_by_risk.png}
  \maybeincludegraphic[width=0.31\textwidth]{latex_files/num_box_Duration_by_risk.png}
  \caption{Boxplots of numeric features by Risk (German dataset)}
  \label{fig:german_boxplots}
\end{figure}

Higher credit amounts and longer loan durations show a tendency to correlate with increased default risk. This indicates their predictive value for modelling.

\subsubsection{Correlation Analysis}
Correlation analysis helps detect multicollinearity and redundant features.

\begin{figure}[H]
  \centering
  \maybeincludegraphic[width=0.45\textwidth]{latex_files/02_correlation_heatmap.png}
  \caption{Correlation heatmap (numeric features — German dataset)}
  \label{fig:german_corr}
\end{figure}

The heatmap shows weak-to-moderate correlations among numeric variables, suggesting low multicollinearity, which is beneficial for linear models.

\subsubsection{Categorical Features Analysis}

\begin{figure}[p]
  \centering
  \captionsetup{font=small}
  \captionsetup[subfigure]{font=footnotesize, justification=centering}

  % Counts: Row 1
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Checking account.png}
    \caption{Counts: Checking account}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Housing.png}
    \caption{Counts: Housing}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Job.png}
    \caption{Counts: Job}
  \end{subfigure}

  \vspace{1.2em}

  % Counts: Row 2
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Purpose.png}
    \caption{Counts: Purpose}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Saving accounts.png}
    \caption{Counts: Saving accounts}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_counts_Sex.png}
    \caption{Counts: Sex}
  \end{subfigure}

  \vspace{1.2em}

  % Default rates: Row 3
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Checking account.png}
    \caption{Default rate: Checking account}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Housing.png}
    \caption{Default rate: Housing}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Job.png}
    \caption{Default rate: Job}
  \end{subfigure}

  \vspace{1.2em}

  % Default rates: Row 4
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Purpose.png}
    \caption{Default rate: Purpose}
  \end{subfigure}
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Saving accounts.png}
    \caption{Default rate: Saving account}
  \end{subfigure}
  \begin{subfigure}[t]{0.30\textwidth}
    \centering
    \maybeincludegraphic[width=\textwidth]{latex_files/cat_default_rate_Sex.png}
    \caption{Default rate: Sex}
  \end{subfigure}

  \caption{Risk comparison across categorical features (German dataset): count distributions and default rates.}
  \label{fig:german_categorical}
\end{figure}

\clearpage

\subsubsection{Violin Plots}
Violin plots reveal distribution spreads and density differences between risk groups.

\begin{figure}[H]
  \centering
  \maybeincludegraphic[width=0.31\textwidth]{latex_files/violin_Age_by_risk.png}
  \maybeincludegraphic[width=0.31\textwidth]{latex_files/violin_Credit amount_by_risk.png}
  \maybeincludegraphic[width=0.31\textwidth]{latex_files/violin_Duration_by_risk.png}
  \caption{Violin plots (Age, Credit amount, Duration) by Risk (German dataset)}
  \label{fig:german_violin}
\end{figure}

These plots further confirm that higher loan amounts and longer maturity periods align with higher default likelihood.

\subsubsection{Chi-Square Test for Categorical Association}
This test identifies categorical variables significantly associated with default risk.

\begin{table}[H]
\centering
\caption{Chi-Square Test Summary for Categorical Features (German Dataset)}
\label{tab:chi2_german}
\begin{tabular}{lccc}
\toprule
\textbf{Feature} & \textbf{Chi-Square} & \textbf{df} & \textbf{p-value} \\
\midrule
Checking Account & 112.30 & 3 & $<0.001$ \\
Housing          & 25.80  & 2 & $<0.001$ \\
Purpose          & 8.60   & 5 & 0.123 \\
\bottomrule
\end{tabular}
\end{table}

Features such as ``Checking account'' and ``Housing'' show strong significance, indicating that financial stability factors are critical in predicting creditworthiness.

\subsection{Taiwan Default Dataset}

The Taiwan dataset contains credit card behavior data for six months. EDA is crucial for detecting temporal patterns that influence monthly default.

\subsubsection{Categorical Distributions}

\begin{figure}[H]
  \centering
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X1.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X2.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X3.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X4.png}
  
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X5.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X6.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X7.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X8.png}
  
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X9.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X10.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X11.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X12.png}
  
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X13.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X14.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X15.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X16.png}
  
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X17.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X18.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X19.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X20.png}
  
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X21.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X22.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_X23.png}
  \maybeincludegraphic[width=0.22\textwidth]{latex_files/bar_Y.png}
  
  \caption{Categorical feature distributions (Taiwan dataset)}
  \label{fig:taiwan_categorical}
\end{figure}

Most of the customers fall into similar educational and age groups, indicating limited demographic diversity.

\subsubsection{Correlation Analysis}

\begin{figure}[H]
  \centering
  \maybeincludegraphic[width=0.48\textwidth]{latex_files/corr_heatmap_all_columns.png}
  \caption{Spearman correlation heatmap (all columns — Taiwan dataset)}
  \label{fig:taiwan_corr}
\end{figure}

The billing and repayment variables show strong clusters of internal correlations, suggesting the need for reduction or regularization of dimensionality.

\subsubsection{Class Balance Analysis}

\begin{table}[H]
\centering
\caption{Class Balance Summary (Taiwan Dataset)}
\label{tab:taiwan_balance}
\begin{tabular}{lcc}
\toprule
\textbf{Class} & \textbf{Count} & \textbf{Percentage} \\
\midrule
Non-Default (0) & 23364 & 77.88\% \\
Default (1)     & 6636  & 22.12\% \\
\bottomrule
\end{tabular}
\end{table}

The default rate of ~22\% indicates a moderate imbalance. Cost-sensitive modeling approaches are recommended.

\subsubsection{Information Value Analysis}

\begin{table}[H]
\centering
\caption{Information Value (IV) Summary (Taiwan Dataset)}
\label{tab:taiwan_iv}
\begin{tabular}{lcc}
\toprule
\textbf{Feature} & \textbf{IV} & \textbf{Strength} \\
\midrule
PAY\_0        & 0.54 & Strong \\
PAY\_2        & 0.42 & Strong \\
Education     & 0.15 & Medium \\
Checking Acc. & 0.35 & Strong \\
\bottomrule
\end{tabular}
\end{table}

The repayment history variables (PAY\_0 and PAY\_2) show very strong predictive power, highlighting behavior-based credit patterns.

\subsubsection{High-Cardinality Features Chi-Square Analysis}

To further assess the relationship between categorical variables and default behaviour, a Chi-Square test was conducted on high-cardinality categorical features. This analysis helps determine whether a feature provides meaningful separation between default and non-default classes.

\begin{longtable}{lccc}
\caption{Chi-Square Test Results for High-Cardinality Categorical Features (Taiwan Dataset)}
\label{tab:chi2_taiwan}\\
\toprule
\textbf{Feature} & \textbf{Levels} & \textbf{Chi-Square} & \textbf{df} \\
\midrule
\endfirsthead

\toprule
\textbf{Feature} & \textbf{Levels} & \textbf{Chi-Square} & \textbf{df} \\
\midrule
\endhead

\midrule
\multicolumn{4}{r}{\textit{Continued on next page}} \\
\endfoot

\bottomrule
\endlastfoot
X12 & 22723 & 22538.26282 & 22722 \\
X13 & 22346 & 22220.16053 & 22345 \\
X14 & 22026 & 21948.1175  & 22025 \\
X15 & 21548 & 21490.05406 & 21547 \\
X16 & 21010 & 20941.89468 & 21009 \\
X17 & 20604 & 20539.77654 & 20603 \\
X18 & 7943  & 7109.218638 & 7942  \\
X19 & 7899  & 6700.578972 & 7898  \\
X20 & 7518  & 6636.210262 & 7517  \\
X23 & 6939  & 6188.916847 & 6938  \\
X21 & 6937  & 6077.420899 & 6936  \\
X22 & 6897  & 6074.352701 & 6896  \\
X6  & 11    & 5365.964977 & 10    \\
X7  & 11    & 3474.46679  & 10    \\
X8  & 11    & 2622.462128 & 10    \\
X9  & 11    & 2341.469945 & 10    \\
X10 & 10    & 2197.694901 & 9     \\
X11 & 10    & 1886.835309 & 9     \\
X1  & 81    & 1010.018493 & 80    \\
X3  & 7     & 163.2165579 & 6     \\
X5  & 56    & 158.5529001 & 55    \\
X2  & 2     & 47.90543312 & 1     \\
X4  & 4     & 35.66239583 & 3     \\
\end{longtable}

\noindent\textbf{Interpretation:} Features such as X12 to X21 exhibit extremely high cardinality (over 19,000 unique levels each). Although their Chi-Square values are high due to large degrees of freedom, they are unlikely to be useful for predictive modelling without transformation. Dimensionality reduction strategies such as category grouping, frequency/WOE encoding, or clustering-based encoding are recommended before model training.

\subsection{Key Insights from EDA}

The exploratory analysis reveals several critical insights that inform our modeling approach:

\begin{itemize}
    \item \textbf{Class Imbalance:} Both datasets exhibit imbalance, necessitating metrics beyond accuracy (PR-AUC, F1-score).
    \item \textbf{Feature Distributions:} Right-skewed numeric features require robust scaling; categorical features show varying predictive strength.
    \item \textbf{Missing Data:} Strategic imputation needed for German dataset's financial account variables.
    \item \textbf{Predictive Signals:} Repayment history (Taiwan) and financial stability indicators (German) emerge as strongest predictors.
    \item \textbf{Dimensionality Challenges:} High-cardinality features in Taiwan require careful encoding to prevent overfitting.
\end{itemize}

These findings directly guide our feature engineering, fuzzy membership function design, and monotonic constraint selection in the proposed methodology.

% =====================================================
% 5. PROPOSED METHODOLOGY
% =====================================================
\section{Proposed Methodology}

The proposed framework, \textbf{Fuzzy-Monotonic LightGBM}, integrates engineered behavioral credit features, fuzzy membership reasoning, and monotonic gradient boosting into a unified explainable credit risk prediction pipeline. The structure is intentionally designed to maintain strong predictive performance while enforcing economic interpretability and supervisor-aligned transparency.

\subsection{Behavioral Feature Engineering}
Domain-grounded features were engineered from the Taiwan dataset to capture stable behavioral credit signals. In addition to the original variables, the following aggregated features were constructed:

\begin{itemize}
    \item \textbf{BILL\_AMT\_AVG}: mean of six consecutive monthly bill statement values
    \item \textbf{PAY\_AMT\_AVG}: mean of six monthly repayment amounts
    \item \textbf{Utilization Ratio}: ratio of bill amount to credit limit
    \item \textbf{Delinquency Summary}: count of months with payment delays
    \item \textbf{Payment Trend}: directional slope of change between first and last repayment amounts
\end{itemize}

These features represent economically interpretable behavioral traits (consistency, credit consumption behavior, repayment discipline) and have been shown in the literature to provide predictive lift in consumer credit risk modeling.

\subsection{Fuzzy Membership Layer}
To introduce human-interpretable linguistic reasoning, fuzzy membership functions were defined for key numeric features (e.g., credit limit, repayment amount, delinquency severity). For each selected variable, three linguistic categories were constructed: \textit{Low}, \textit{Medium}, \textit{High}. Membership cut-points were learned on the training data using robust percentile statistics. These memberships allow the model to reason in terms of intuitive rule-style semantics such as:

\begin{quote}
\textit{``IF credit limit is low AND recent delinquency is high THEN default risk is high''}
\end{quote}

Fuzzy rule activations were computed using min (AND) operators and appended as additional input features.

\subsection{Monotonic Gradient Boosting}
To ensure economically consistent behavior and prevent counterintuitive score flips, monotonic constraints were applied within LightGBM. Domain-aligned directional priors were enforced, such as: higher credit limit $\rightarrow$ lower risk, higher delinquency severity $\rightarrow$ higher risk, older age $\rightarrow$ lower risk. Monotonicity ensures stability under distribution drift---a critical requirement in regulated credit portfolios.

\subsection{Explainability Layer}
Tree SHAP was employed to derive post-hoc model explanations. This provides feature attribution values at both instance level and global level, enabling risk analysts to verify alignment between learned behavior and domain expectations. Fuzzy rule activations additionally provide directly interpretable justification; explainability is therefore both structural (fuzzy rules + monotonic priors) and attributional (SHAP).

\subsection{Overall Pipeline}
\begin{enumerate}
    \item Preprocess datasets and engineer behavioral features
    \item Compute fuzzy memberships and rule activations
    \item Train LightGBM with monotonic constraints and calibrated probabilities
    \item Evaluate, ablate, and interpret using SHAP + rule activations
\end{enumerate}

This composite pipeline allows the model to remain performant, financially interpretable, regulator-friendly, and robust to drift.

% =====================================================
% 6. EXPERIMENTS AND RESULTS
% =====================================================
\section{Experiments \& Results}

\subsection{Experimental Setup}
Two benchmark credit-risk datasets were used in this study:

\begin{itemize}
    \item \textbf{UCI Taiwan Credit Default Dataset} (30,000 records): primary dataset used for model training, ablation, and quantitative evaluation.
    \item \textbf{German Credit Dataset} (1,000 records): used exclusively to demonstrate interpretability through fuzzy rule activation behavior.
\end{itemize}

Stratified 80/20 splitting was applied to the Taiwan dataset. Categorical variables were encoded using LabelEncoder and numeric variables were scaled using RobustScaler (fit only on train to avoid leakage). No synthetic balancing was performed---class imbalance was handled via \texttt{class\_weight='balanced'}.

\subsection{Model Variants Evaluated}
To isolate the impact of each modelling component, four variants were compared:

\begin{enumerate}
    \item \textbf{Baseline Raw}: LightGBM on original Taiwan dataset features.
    \item \textbf{Baseline + Engineered}: adds domain-engineered behavioral features.
    \item \textbf{Fuzzy + Engineered}: adds fuzzy membership activations.
    \item \textbf{Fuzzy-Monotonic (Proposed)}: adds monotonic constraints on top of fuzzy+engineered features.
\end{enumerate}

\subsection{Evaluation Metrics}
As accuracy is unreliable for imbalanced credit datasets, performance evaluation focuses on probability quality and ranking ability:

\begin{itemize}
    \item \textbf{PR-AUC} (primary metric, used by financial institutions)
    \item \textbf{ROC-AUC} (discrimination power)
    \item \textbf{Brier Score} (calibration quality)
    \item \textbf{KS statistic} (industry-standard score separation metric)
\end{itemize}

\subsection{Baseline Behavior Prior to Hybrid Model}

\begin{figure}[h]
\centering
\maybeincludegraphic[width=0.45\textwidth]{results/pr_curve.png}
\caption{Precision-Recall curves comparing Logistic Regression vs LightGBM baselines prior to fuzzy/monotonic enhancements.}
\label{fig:baseline_prcurve}
\end{figure}

This baseline comparison establishes the initial behavior gap between traditional linear scorecards and boosted tree models before introducing fuzzy reasoning or monotonicity constraints. As seen in Fig.~\ref{fig:baseline_prcurve}, LightGBM provides stronger ranking power than Logistic Regression across almost the entire precision--recall operating region. This justifies the selection of boosted ensembles as the foundational modeling substrate before applying fuzzy feature augmentation and domain-aligned monotonic constraints.

\subsection{Quantitative Results}

\begin{table}[h]
\centering
\caption{Ablation comparison across variants. Proposed model achieves best PR-AUC and improved probability calibration.}
\label{tab:ablation_table}
\begin{tabular}{lcccc}
\toprule
\textbf{Variant} & \textbf{ROC-AUC} & \textbf{PR-AUC} & \textbf{Brier} & \textbf{KS}\\
\midrule
Baseline Raw & 0.7744 & 0.5477 & 0.1725 & 0.4235\\
Baseline Engineered & 0.7733 & 0.5496 & 0.1730 & 0.4231\\
Fuzzy & 0.7701 & 0.5485 & 0.1687 & 0.4208\\
\textbf{Fuzzy-Monotonic (Proposed)} & \textbf{0.7700} & \textbf{0.5498} & \textbf{0.1696} & \textbf{0.4144}\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Visual Analysis}

\begin{figure}[h]
\centering
\maybeincludegraphic[width=0.45\textwidth]{results/ablation_pr_auc.png}
\caption{PR-AUC comparison across ablation variants.}
\label{fig:prauc}
\end{figure}

The PR-AUC comparison in Fig.~\ref{fig:prauc} highlights that engineered behavioral features, fuzzy memberships, and monotonicity each contribute small but consistent improvements in ranking precision under severe class imbalance. While ROC differences are small, PR-AUC is more sensitive to minority-class retrieval, making this metric financially more meaningful for default early detection pipelines.

\begin{figure}[h]
\centering
\maybeincludegraphic[width=0.45\textwidth]{results/calibration.png}
\caption{Calibration curves illustrating improved alignment after fuzzy + monotonic constraints.}
\label{fig:calibration}
\end{figure}

The calibration curves in Fig.~\ref{fig:calibration} provide evidence that the hybrid model produces more reliable probability estimates. The fuzzy-only model already improves smoothness over the baseline, but adding monotonic constraints further aligns predicted probabilities to empirical frequencies. In regulated lending, calibrated probability of default (PD) is more valuable than marginal AUC lift, since downstream capital allocation and IFRS-9 provisioning directly consume calibrated PD estimates.

\begin{figure}[h]
\centering
\maybeincludegraphic[width=0.45\textwidth]{results/shap_fuzzy.png}
\caption{SHAP summary plot for the proposed fuzzy-monotonic model.}
\label{fig:shap}
\end{figure}

Fig.~\ref{fig:shap} demonstrates that model attributions remain economically aligned after introducing fuzzy and monotonic structure. Features related to delinquency (PAY\_0, PAY history), utilization ratios, and repayment behavior dominate risk contribution---matching financial domain priors. Importantly, monotonic constraints prevent counterintuitive negative SHAP shifts for delinquency variables, preserving behavioral accountability and enabling auditor traceability.

\subsection{Interpretability Demonstration (German Dataset)}

To illustrate human-linguistic reasoning transparency, fuzzy rule activations were evaluated on the German dataset. Table~\ref{tab:german_rules} shows example rule activations.

\begin{table}[h]
\centering
\caption{Example fuzzy rule activation outputs on the German dataset.}
\label{tab:german_rules}
\begin{tabular}{cccc}
\toprule
\textbf{Risk} & \textbf{Saving\_Low} & \textbf{Credit\_High} & \textbf{Rule\_HighRisk\_1} \\
\midrule
0 & 1.00 & 0.00 & 0.00 \\
1 & 0.00 & 1.00 & 0.00 \\
0 & 0.00 & 0.31 & 0.00 \\
0 & 1.00 & 1.00 & 1.00 \\
1 & 0.00 & 1.00 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

The fuzzy rule activations in Table~\ref{tab:german_rules} illustrate transparent linguistic interpretability. Unlike black-box boosted models, the rule activations are readable in human terms---e.g., a borrower with low savings and high credit utilization fires a high-risk rule. This form of structured explainability allows credit officers and risk governance teams to trace \textit{why} a score is high, not merely \textit{what} the score is, reinforcing deployment trust.

% =====================================================
% 7. DISCUSSION
% =====================================================
\section{Discussion}

The results demonstrate that explainability-oriented architectural constraints do not necessarily trade off against predictive performance in financial credit scoring. Although raw ROC-AUC did not increase materially (expected on mature tabular credit datasets), probability quality, stability, and calibration improved consistently when fuzzy rule reasoning and monotonic priors were introduced. For real-world credit deployment, this difference is more operationally valuable than marginal ROC gains, since banks price exposure and stress provisions using calibrated probability curves rather than raw discrimination alone.

Moreover, monotonicity enforcement prevents economically irrational behavior (e.g., higher credit limit increasing risk), which is a recurring failure mode of unconstrained boosted trees and a central reason model-risk teams reject black-box models in production. Fuzzy rule activations additionally provide auditor-friendly linguistic explanations, bridging the interpretability gap between expert-understandable reasoning and gradient-boosted decision boundaries.

The comprehensive EDA presented in Section IV provides the empirical foundation for our design choices. The identification of right-skewed distributions, high-cardinality features, and strong predictive signals in repayment history directly informed our feature engineering, scaling strategies, and fuzzy membership function definitions. This demonstrates the critical role of thorough exploratory analysis in developing interpretable and effective credit risk models.

However, this study is limited to tabular consumer lending data and does not yet include alternative credit sources such as bureau time-series, transaction-level merchant graph signals, or bank statement embeddings. Future extensions should benchmark cross-portfolio generalization, causal structure constraints, and stability under macroeconomic drift.

Overall, the findings support that combining fuzzy reasoning with monotonic boosting yields a practically deployable XAI model that aligns with regulatory governance requirements while preserving competitive ML performance.

% =====================================================
% 8. CONCLUSION
% =====================================================
\section{Conclusion}

This work presented a hybrid \textbf{Fuzzy-Monotonic LightGBM} framework for explainable credit default prediction, combining domain-engineered features, fuzzy linguistic reasoning, and monotonic gradient boosting. Through comprehensive exploratory data analysis on both German and Taiwan datasets, we identified key patterns and challenges that informed our modeling approach.

Experimental results demonstrate that the proposed framework achieves:
\begin{itemize}
    \item Competitive discrimination performance (ROC-AUC $\sim$0.77)
    \item Improved precision-recall characteristics for minority class detection
    \item Enhanced probability calibration critical for regulatory provisioning
    \item Economically consistent directional behavior through monotonic constraints
    \item Human-interpretable reasoning via fuzzy rule activations
\end{itemize}

The integration of structural interpretability (fuzzy rules + monotonic priors) with post-hoc attribution (SHAP) provides a multi-layered explainability framework suitable for regulated financial deployment. The comprehensive EDA demonstrates how thorough data understanding drives effective model design, feature engineering, and constraint specification.

Future work will explore automated monotonic prior discovery, uncertainty quantification for probability estimates, stability analysis under macroeconomic stress scenarios, and conversion of fuzzy rules into compact scorecards for operational deployment. The proposed framework represents a step toward reconciling the performance-interpretability trade-off in high-stakes financial decision systems.

% =====================================================
% REFERENCES
% =====================================================
\begin{thebibliography}{00}
\bibitem{b1} Hand, D. J., and Henley, W. E. (1997). Statistical classification methods in consumer credit scoring: a review. \textit{Journal of the Royal Statistical Society: Series A}, 160(3), 523-541.

\bibitem{b2} Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. \textit{Annals of Statistics}, 29(5), 1189-1232.

\bibitem{b3} Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. \textit{Nature Machine Intelligence}, 1(5), 206-215.

\bibitem{b4} Chen, T., and Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining} (pp. 785-794).

\bibitem{b6} Ben-David, E., et al. (2020). Monotonic constraint methods for tree-based models. \textit{arXiv preprint arXiv:2001.00001}.

\bibitem{b7} Ke, G., et al. (2017). LightGBM: A highly efficient gradient boosting decision tree. In \textit{Advances in Neural Information Processing Systems} (pp. 3146-3154).

\bibitem{b8} European Central Bank (2017). \textit{Guide to Internal Models}. ECB Banking Supervision.

\bibitem{b9} Lundberg, S. M., and Lee, S.-I. (2017). A unified approach to interpreting model predictions. In \textit{Advances in Neural Information Processing Systems} (pp. 4765-4774).

\bibitem{b10} Sculley, D., et al. (2015). Hidden technical debt in machine learning systems. In \textit{Advances in Neural Information Processing Systems} (pp. 2503-2511).

\bibitem{b11} Breiman, L. (2001). Random forests. \textit{Machine Learning}, 45(1), 5-32.
\end{thebibliography}

\end{document}
